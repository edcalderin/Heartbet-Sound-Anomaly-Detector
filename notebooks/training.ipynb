{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hearbet Sound Anomaly Detector - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = Path('../unzipped_data/')\n",
    "\n",
    "def list_files():\n",
    "    for file in AUDIO_DIR.glob('**/*.wav'):\n",
    "        yield file.as_posix()\n",
    "\n",
    "def create_dataframe():     \n",
    "    data_files = []\n",
    "    for filename in list_files():\n",
    "        if filename.find('artifact')>-1:\n",
    "            data_files.append((filename, 'artifact'))\n",
    "        if filename.find('extrahls')>-1:\n",
    "            data_files.append((filename, 'extrahls'))\n",
    "        if filename.find('extrastole')>-1:\n",
    "            data_files.append((filename, 'extrastole'))\n",
    "        if filename.find('murmur')>-1:\n",
    "            data_files.append((filename, 'murmur'))\n",
    "        if filename.find('normal')>-1:\n",
    "            data_files.append((filename, 'normal'))\n",
    "    \n",
    "    return pd.DataFrame(data_files, columns=('fname', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = create_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artifact': 0, 'extrahls': 1, 'murmur': 2, 'normal': 3, 'extrastole': 4}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataframe.label.unique()\n",
    "dict(zip(labels, range(len(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, audio_length: float, target_sample_rate: int) -> None:\n",
    "        self.dataframe = dataframe\n",
    "        self.audio_length = audio_length\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = target_sample_rate * audio_length\n",
    "        self.labels = dataframe['label'].values\n",
    "        self.filenames = dataframe['fname'].values\n",
    "        self.class_indices = {'artifact': 0, 'extrahls': 1, 'murmur': 2, 'normal': 3, 'extrastole': 4}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        waveform, sample_rate = torchaudio.load(self.filenames[index])\n",
    "        waveform = torch.mean(waveform, axis=0)\n",
    "        \n",
    "        if sample_rate!=self.target_sample_rate:\n",
    "            resampler = T.Resample(sample_rate, self.target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[0] > self.num_samples:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "        else:\n",
    "            waveform = F.pad(waveform, (0, self.num_samples - waveform.shape[0]))\n",
    "        melspectrgoram = T.MelSpectrogram(n_fft=128, n_mels=128, hop_length=128)\n",
    "        melspec = melspectrgoram(waveform)\n",
    "        \n",
    "        class_idx = self.class_indices[self.labels[index]]\n",
    "        \n",
    "        return torch.stack([melspec]), class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUDIO_LENGTH: int = 15\n",
    "TARGET_SAMPLE_RATE: int = 4000\n",
    "\n",
    "audio_dataset = AudioDataset(dataframe, AUDIO_LENGTH, TARGET_SAMPLE_RATE)\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(audio_dataset, [.7, .3], generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 410 -- Test: 175\n"
     ]
    }
   ],
   "source": [
    "print('Train:', len(train_dataset.indices), '-- Test:', len(test_dataset.indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE: int = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES: int = 5\n",
    "INPUT_SHAPE = (128, 469, 1)\n",
    "\n",
    "class HearbetDetectorModel(nn.Module):\n",
    "    def __init__(self, num_classes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 4)),\n",
    "            nn.MaxPool2d(kernel_size=(2, 3), stride=(2, 3)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 4)),\n",
    "            nn.MaxPool2d(kernel_size=(2, 3), stride=(2, 3)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 4)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(64*28*47, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.conv_block1(X)\n",
    "        X = self.conv_block2(X)\n",
    "        X = self.conv_block3(X)\n",
    "        X = X.view(-1, 64*28*47)\n",
    "        X = self.dense_layers(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HearbetDetectorModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 4), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 4), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (conv_block3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (dense_layers): Sequential(\n",
       "    (0): Linear(in_features=84224, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HearbetDetectorModel(NUM_CLASSES)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 0.5849769539557971\n",
      "Epoch: 1 | Training loss: 0.5971282754953091\n",
      "Epoch: 2 | Training loss: 0.5902307572273108\n",
      "Epoch: 3 | Training loss: 0.5602060728348218\n",
      "Epoch: 4 | Training loss: 0.5484531338398273\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = []\n",
    "    for specs, labels in train_loader:\n",
    "        outputs = model(specs)\n",
    "        loss = loss_fn(outputs, labels) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "    print(f'Epoch: {epoch} | Training loss: {np.mean(train_loss)}')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heartbet-sound-anomaly-detector-JmO5NjpB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
