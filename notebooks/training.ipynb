{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hearbet Sound Anomaly Detector - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2+cu121'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = Path('../unzipped_data/')\n",
    "\n",
    "def list_files():\n",
    "    for file in AUDIO_DIR.glob('**/*.wav'):\n",
    "        yield file.as_posix()\n",
    "\n",
    "def create_dataframe():\n",
    "    data_files = []\n",
    "    for filename in list_files():\n",
    "        if filename.find('artifact')>-1:\n",
    "            data_files.append((filename, 'artifact'))\n",
    "        if filename.find('extrahls')>-1:\n",
    "            data_files.append((filename, 'extrahls'))\n",
    "        if filename.find('extrastole')>-1:\n",
    "            data_files.append((filename, 'extrastole'))\n",
    "        if filename.find('murmur')>-1:\n",
    "            data_files.append((filename, 'murmur'))\n",
    "        if filename.find('normal')>-1:\n",
    "            data_files.append((filename, 'normal'))\n",
    "\n",
    "    return pd.DataFrame(data_files, columns=('fname', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = create_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artifact': 0, 'extrahls': 1, 'murmur': 2, 'normal': 3, 'extrastole': 4}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = dataframe.label.unique()\n",
    "dict(zip(labels, range(len(labels))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, audio_length: float, target_sample_rate: int) -> None:\n",
    "        self.dataframe = dataframe\n",
    "        self.audio_length = audio_length\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = target_sample_rate * audio_length\n",
    "        self.labels = dataframe['label'].values\n",
    "        self.filenames = dataframe['fname'].values\n",
    "        self.class_indices = {'artifact': 0, 'extrahls': 1, 'murmur': 2, 'normal': 3, 'extrastole': 4}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        waveform, sample_rate = torchaudio.load(self.filenames[index])\n",
    "        waveform = torch.mean(waveform, axis=0)\n",
    "        \n",
    "        if sample_rate!=self.target_sample_rate:\n",
    "            resampler = T.Resample(sample_rate, self.target_sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        if waveform.shape[0] > self.num_samples:\n",
    "            waveform = waveform[:self.num_samples]\n",
    "        else:\n",
    "            waveform = F.pad(waveform, (0, self.num_samples - waveform.shape[0]))\n",
    "        melspectrgoram = T.MelSpectrogram(n_fft=128, n_mels=128, hop_length=128)\n",
    "        melspec = melspectrgoram(waveform)\n",
    "        \n",
    "        class_idx = self.class_indices[self.labels[index]]\n",
    "        \n",
    "        return torch.stack([melspec]), class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "AUDIO_LENGTH: int = 15\n",
    "TARGET_SAMPLE_RATE: int = 4000\n",
    "\n",
    "audio_dataset = AudioDataset(dataframe, AUDIO_LENGTH, TARGET_SAMPLE_RATE)\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(audio_dataset, [.7, .3], generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 410 -- Test: 175\n"
     ]
    }
   ],
   "source": [
    "print('Train:', len(train_dataset.indices), '-- Test:', len(test_dataset.indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataframe, audio_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_CLASSES: 5\n",
    "# INPUT_SHAPE: (128, 469, 1)\n",
    "\n",
    "class HearbetDetectorModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(3, 4)),\n",
    "            nn.MaxPool2d(kernel_size=(2, 3), stride=(2, 3)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 4)),\n",
    "            nn.MaxPool2d(kernel_size=(2, 3), stride=(2, 3)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 4)),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(64*28*47, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 5)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.conv_block1(X)\n",
    "        X = self.conv_block2(X)\n",
    "        X = self.conv_block3(X)\n",
    "        X = X.view(-1, 64*28*47)\n",
    "        X = self.dense_layers(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HearbetDetectorModel(\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 4), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 4), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=(2, 3), stride=(2, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (conv_block3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 4), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (dense_layers): Sequential(\n",
      "    (0): Linear(in_features=84224, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(HearbetDetectorModel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class HearbetDetectorNetwork:\n",
    "    def __init__(self, model, lr: float) -> None:\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = model.to(self.device)\n",
    "        self.__optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "        self.__precision_fn: Callable = Precision(task='multiclass', num_classes=5).to(self.device)\n",
    "\n",
    "    def __init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            module.bias.data.fill_(0.01)\n",
    "\n",
    "    def __get_report_by_epoch(self, *args) -> None:\n",
    "        print('Epoch: {} | Train loss: {:.4f} - Train precision: {:.4f} --- Val loss: {:.4f} - Val precision: {:.4f}'.format(*args))\n",
    "\n",
    "    def train(self, loss_function, epochs: int, batch_size: int, training_set, validation_set):\n",
    "        self.model.apply(self.__init_weights)\n",
    "\n",
    "        train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(validation_set, batch_size=batch_size)\n",
    "\n",
    "        print('Training on', self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            for specs, labels in train_loader:\n",
    "                specs, labels = specs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(specs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                self.__optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.__optimizer.step()\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                self.model.eval()\n",
    "                train_precision = 0\n",
    "                for specs, labels in train_loader:\n",
    "                    specs, labels = specs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(specs)\n",
    "                    train_precision += self.__precision_fn(outputs, labels)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                self.model.eval()\n",
    "                val_precision, val_loss = 0, 0\n",
    "                for specs, labels in val_loader:\n",
    "                    specs, labels = specs.to(self.device), labels.to(self.device)\n",
    "                    outputs = self.model(specs)\n",
    "                    loss = loss_function(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    val_precision += self.__precision_fn(outputs, labels)\n",
    "\n",
    "            self.__get_report_by_epoch(epoch + 1,\n",
    "                                       train_loss/len(train_loader),\n",
    "                                       train_precision/len(train_loader),\n",
    "                                       val_loss/len(val_loader), \n",
    "                                       val_precision/len(val_loader))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda\n",
      "Epoch: 1 | Train loss: 1.4824 - Train precision: 0.5762 --- Val loss: 1.4547 - Val precision: 0.4583\n",
      "Epoch: 2 | Train loss: 1.0412 - Train precision: 0.6738 --- Val loss: 1.3538 - Val precision: 0.5806\n",
      "Epoch: 3 | Train loss: 0.9300 - Train precision: 0.7071 --- Val loss: 1.4102 - Val precision: 0.6167\n",
      "Epoch: 4 | Train loss: 0.8640 - Train precision: 0.7214 --- Val loss: 1.4957 - Val precision: 0.5917\n",
      "Epoch: 5 | Train loss: 0.8170 - Train precision: 0.7333 --- Val loss: 1.4561 - Val precision: 0.5917\n",
      "Epoch: 6 | Train loss: 0.7857 - Train precision: 0.7333 --- Val loss: 1.5030 - Val precision: 0.5833\n",
      "Epoch: 7 | Train loss: 0.7409 - Train precision: 0.7381 --- Val loss: 1.4173 - Val precision: 0.6000\n",
      "Epoch: 8 | Train loss: 0.7133 - Train precision: 0.7667 --- Val loss: 1.5883 - Val precision: 0.5778\n",
      "Epoch: 9 | Train loss: 0.6644 - Train precision: 0.8000 --- Val loss: 1.5146 - Val precision: 0.6083\n",
      "Epoch: 10 | Train loss: 0.6489 - Train precision: 0.7976 --- Val loss: 1.6141 - Val precision: 0.6111\n",
      "Epoch: 11 | Train loss: 0.6045 - Train precision: 0.8095 --- Val loss: 1.5731 - Val precision: 0.6194\n",
      "Epoch: 12 | Train loss: 0.5771 - Train precision: 0.7381 --- Val loss: 1.6909 - Val precision: 0.4917\n",
      "Epoch: 13 | Train loss: 0.5802 - Train precision: 0.8167 --- Val loss: 1.6223 - Val precision: 0.6472\n",
      "Epoch: 14 | Train loss: 0.5474 - Train precision: 0.7786 --- Val loss: 1.8244 - Val precision: 0.5222\n",
      "Epoch: 15 | Train loss: 0.5311 - Train precision: 0.8357 --- Val loss: 1.7540 - Val precision: 0.5722\n"
     ]
    }
   ],
   "source": [
    "hearbetDetectorNetwork = HearbetDetectorNetwork(HearbetDetectorModel(), lr=0.001)\n",
    "\n",
    "hearbetDetectorNetwork.train(nn.CrossEntropyLoss(), epochs=15, batch_size=15, training_set=train_dataset, validation_set=test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heartbet-sound-anomaly-detector-JmO5NjpB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
